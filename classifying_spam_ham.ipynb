{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Data from https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_labels</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_labels                                            message\n",
       "0          ham  Go until jurong point, crazy.. Available only ...\n",
       "1          ham                      Ok lar... Joking wif u oni...\n",
       "2         spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3          ham  U dun say so early hor... U c already then say...\n",
       "4          ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open for reading with \"universal\" type set\n",
    "# reading the file and parsing with tab delimited \n",
    "doc = codecs.open('SMSSpamCollection','rU','UTF-8') \n",
    "df = pd.read_csv(doc, sep='\\t', header=None, names=['email_labels', 'message'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go jurong point, crazy.. available bugis n gre...\n",
       "1                        ok lar... joking wif u oni...\n",
       "2    free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3                    u dun say early hor... u c say...\n",
       "4                          nah i think goes usf, lives\n",
       "Name: processed_message, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing the message columns by removing stopwords, punctuation, lowercase\n",
    "english_stop_words = set(stopwords.words('english') ).union( set(ENGLISH_STOP_WORDS))\n",
    "df['processed_message'] = df['message'].apply(lambda v: [k for k in v.split() if k not in english_stop_words])\n",
    "df['processed_message'] = df['processed_message'].apply(lambda v: [k for k in v if k not in string.punctuation])\n",
    "df['processed_message'] = df['processed_message'].apply(lambda v: ' '.join([k.lower() for k in v]))\n",
    "df['processed_message'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 4825)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate out spam, ham messages\n",
    "# get frequencey of words to understand which words are contributing to spam\n",
    "df_spam = df[df['email_labels'] == 'spam']\n",
    "df_ham = df[df['email_labels'] == 'ham']\n",
    "len(df_spam['processed_message']), len(df_ham['processed_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim         106\n",
       "prize          58\n",
       "urgent!        43\n",
       "tone           40\n",
       "awarded        38\n",
       "£1000          33\n",
       "150ppm         30\n",
       "guaranteed     29\n",
       "entry          26\n",
       "ringtone       24\n",
       "4*             24\n",
       "tones          24\n",
       "valid          23\n",
       "500            23\n",
       "£100           22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_words = df_spam['processed_message'].str.split(expand=True).stack().value_counts()\n",
    "spam_unique_words = spam_words.keys()\n",
    "ham_unique_words = df_ham['processed_message'].str.split(expand=True).stack().value_counts().keys()\n",
    "spam_inc_words = spam_words.loc[set(spam_unique_words) - set(ham_unique_words) ].sort_values(ascending=False).head(15)\n",
    "spam_inc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above spam words makes sense as message with subject claim your prize, you have won this much, \n",
    "# congrats you are awarded are most likely to be spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try to predict the email labels based on preprocessed message\n",
    "x = df['processed_message']\n",
    "y = df['email_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function which takes the train and test data along with the classifier model that we choose to use\n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    " \n",
    "    classifier.fit(X_train, y_train)\n",
    "    print \"Accuracy: %s\" % classifier.score(X_test, y_test)\n",
    "    predicted = classifier.predict(X_test)\n",
    "    return classifier, predicted, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9734386216798278\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', clf),\n",
    "])\n",
    " \n",
    "classifier, predicted, X_test, y_test = train(trial1, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      1.00      0.98      1195\n",
      "       spam       1.00      0.81      0.90       198\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(y_test, predicted, target_names=df['email_labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1195    0]\n",
      " [  37  161]]\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(y_test, predicted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856424982053122\n"
     ]
    }
   ],
   "source": [
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=0.03)),\n",
    "])\n",
    " \n",
    "classifier, predicted, X_test, y_test = train(trial1, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      1.00      0.99      1195\n",
      "       spam       0.97      0.92      0.95       198\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(y_test, predicted, target_names=df['email_labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1190    5]\n",
      " [  15  183]]\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(y_test, predicted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
